# NFS

NFS存储在集群中用于保存静态资源数据。NFS主要功能是通过*局域网络*在不同主机间共享文件，用于企业集群架构中，如果是大型网站，会用到更复杂的分布式文件系统FastDFS（4K~500M的小文件，音频、小说、视频），Glusterfs（大文件，iso镜像），HDFS

**NFS主要用于解决web静态资源的一致性和资源共享问题**，避免web服务器的磁盘空间浪费，但它不能解决网站访问的延时问题，甚至会扩展大这个问题

## NFS写入原理

[![NFS写入原理](https://s1.ax1x.com/2022/11/26/zNiudK.png)](https://imgse.com/i/zNiudK)

NFS服务端本地写入，当用户执行mkdir命令，该命令会调用shell解释器翻译给内核，内核解析完成后驱动对应的硬件设备

NFS远程写入实现过程

1. 用户进程访问NFS客户端，使用不同的函数封装不同的操作命令
2. NFS客户端通过TCP/IP协议传输数据到NFS服务端
3. NFS服务端接收请求后，会先调用portmap进程进行端口映射
4. nfsd进程用于判断NFS客户端是否拥有权限连接NFS服务端
5. Rpc.mount进程判断客户端是否有具备文件的操作权限
6. idmap进程实现用户的映射和压缩（与NFS配置文件参数有关）
7. 最后NFS服务端会将对应请求的函数解封装转换为本地能识别的命令，传递至内核，由内核驱动硬件

从NFS的远程写入过程中，无论客户端操作系统是什么操作系统平台并不重要，客户端的命令首先都需要交给函数进行封装，函数传输到服务端后再进行识别；nfs服务发起的远程过程调用基于RPC协议，所以使用nfs必须启动rpc服务

- Rpc.nfsd：基本的NFS守护进程，主要功能是管理客户端是否能够登录服务器
- Rpc.mount：主要功能是管理NFS的文件系统。当客户端顺利通过nfsd登录NFS服务器后，在使用NFS服务所提供的文件前，还必须通过文件使用权限的验证。它会读取NFS的配置文件/etc/exports来对比客户端权限
- Portmap：主要功能是进行端口映射工作，其本身默认监听111端口

## NFS服务安装

1. 安装

```shell
[root@nfs ~]# yum install -y nfs-utils	# rpc包会被作为nfs的依赖包一起安装
[root@nfs ~]# rpm -qa rpcbind		# 检查rpc包
[root@nfs ~]# systemctl status rpcbind	# rpc安装后默认会设置为开机自启
```

2. 配置

NFS主配置文件`/etc/exports`，NFS配置文件语法格式：`共享目录 NFS客户端地址(客户端权限)`

示例：将nfs服务端的`/data`目录共享给`172.16.1.0/24`网段的所有主机

1. 所有客户端主机都具备读写权限
2. 将数据写入NFS服务器的硬盘中后才会结束操作，最大限度保证数据不丢失
3. 将所有用户映射为本地的匿名用户（nfsnobody）

```shell
[root@nfs ~]# more /etc/exports
/data	172.16.1.0/24(rw,sync,all_squash)

[root@nfs ~]# mkdir /data
[root@nfs ~]# firewall-cmd --add-service=nfs --permanent
[root@nfs ~]# firewall-cmd --add-service=rpc-bind --permanent
[root@nfs ~]# firewall-cmd --add-service=mountd --permanent
[root@nfs ~]# firewall-cmd --reload
```

3. 启动

```shell
systemctl start nfs-server
systemctl enable nfs-server
```

4. 检测

NFS服务启动后，`/var/lib/nfs/etab`内会记录NFS的共享配置信息，如果此文件为空，则配置文件可能有误，查看此文件时能看到大量权限参数，除了手动配置的参数以外，其他的参数都是NFS默认添加的

```shell
[root@nfs ~]# more /var/lib/nfs/etab
[root@nfs ~]# ss -lntp | column -t	# 查看端口
```

5. 客户端挂载

NFS通过RPC协议进行交互，所以客户端也必须具备rpcbind服务，除此以外，nfs-utils包中仍包含一些客户端要使用到的命令，例如showmount等

```shell
[root@web01 ~]# yum install -y nfs-utils
[root@web01 ~]# showmount -e 172.16.1.31	# 检查是否存在共享内容
[root@web01 ~]# mount -t nfs 172.16.1.31:/data /opt
[root@web01 ~]# df -h	# 查看挂载
```

此时NFS客户端连接挂载完毕，但客户端往共享目录里写入数据时会提示权限错误，客户端连接服务端的时候会经过两层验证，nfsd会检测客户端是否能够连接成功，mount会检测客户端对共享目录的权限。在上例中，mount的权限是rw，这两个检测都没有问题，因此，提示权限错误是由于主配置文件中，客户端的另一个权限参数all_squash导致

all_squash参数表示将所有客户端的用户压缩成一个匿名用户，这个匿名用户的uid和gid都是65534（nfsnobody)。所以，任何一个客户端的指令到达服务端后，都会被NFS服务交给这个匿名用户进行接封装再执行，但由于服务端的共享目录是由root创建，所以匿名用户没有权限操作共享目录，重新更改共享目录的授权即可。关于匿名用户的信息，在`/var/lib/nfs/etab`文件中也有记录

上例挂载属于临时操作，永久挂载需要写入`/etc/fstab`，但该文件一旦写入错误就可能导致设备重启时，系统启动失败，所以写入`/etc/fstab`文件后还需要使用`mount -a`命令测试是否能够正常挂载

## NFS服务端参数

在一般工作场景下，通常NFS服务端共享只是普通的静态数据，不需要执行`suid、exec`等权限，挂载的这个文件系统只能作为存取数据用，无法执行程序，对于服务端或客户端而言也增加了安全性

```shell
# 通过mount -o选项指定挂载参数，增加安全性能
mount -t nfs -o nosuid,noexec,nodev 172.16.1.31:/data /opt

# 禁止更新目录及文件时间戳挂载，一定程度上提升性能
mount -t nfs -o noatime,nodiratime 172.16.1.31:/data /opt
```

这两个选项对于现在的服务器架构而言，基本不需要添加这两个配置，硬盘读写纪录时间戳一定程度上会影响磁盘I/O，CDN能够解决这个问题

### NFS配置详解

|nfs共享参数|参数作用|
|---|---|
|rw|读写权限|
|ro|只读权限|
|root_squash|当NFS客户端以root账户访问时，映射为NFS服务端的匿名用户（不常用）|
|no_root_squash|当NFS客户端以root账户访问时，映射为NFS服务端的root用户（不常用）|
|all_squash|当NFS客户端以任意账户访问时，映射为NFS服务端的匿名用户（不常用）|
|no_all_squash|当NFS客户端以任意账户访问时，都不进行压缩|
|sync|同时将数据写入内存和磁盘中，保证不丢失数据|
|async|优先将数据写入内存，再写入硬盘；效率更高，可能丢失数据|
|anonuid|配合all_squash使用，指定NFS的用户UID，服务端必须存在该用户|
|anongid|配合all_squash使用，指定NFS的用户GID，服务端必须存在该用户|

示例：验证all_squash、anonuid、anongid权限

```shell
[root@nfs ~]# more /etc/exports
/data/   172.16.1.0/24(rw,sync,all_squash,anonuid=666,anongid=666)

[root@nfs ~]# groupadd -g 666 www
[root@nfs ~]# useradd -u 666 -g 666 www
[root@nfs ~]# chown -R www.www /data
[root@nfs ~]# systemctl restart nfs-server.service

[root@web01 ~]# touch file
[root@web01 ~]# echo "hello" > file
[root@web01 ~]# ll /opt
-rw-r--r--. 1 666 666         6 Nov 30 11:43 file
```

NFS的远程写入，请求到达服务端后会切换成UID为666的用户执行写入操作，所以服务端的www用户一定要有能够操作共享目录的权限，而客户端系统没有UID为666的用户，所以客户端在查看共享目录属性信息的时候，所属组和所属用户都是直接显示数字，而不是显示用户名

### NFS优缺点

NFS存储优点

1. NFS文件系统简单易用、方便部署
2. NFS存放的数据都在文件系统之上，所有数据直观可见

NFS存储局限

1. 存在单点故障，如果构建高可用，维护又比较麻烦 web -> nfs(sersync) -> backup
2. NFS数据明文，不对数据做任何校验
3. 客户端挂载NFS没有密码验证，一般只在内网使用

## Sersync实时同步

只要当前目录发生变化，则会触发一个事件，事件触发后将产生变化的数据同步至远程服务器，这就是实时同步。数据实时同步配合NFS使用，能够解决NFS单点故障问题，除此以外还具备保证数据的连续性、减少人力维护成本等优势

实时同步的实现，需要借助`Inotify`通知接口，用于监控目录的变化，如果被监控的目录发生变更，则触发动作，这个动作可以是进行一次同步操作或其他操作。实时同步工具的选择有`sersync`、`inotify+rsync`，其中sersync是基于`rsync+inotify-tools`开发的工具，其强化了实时监控、文件过滤、简化配置等功能，帮助用户提高运行效率，节省时间和网络资源

[sersync工具](https://github.com/wsgzao/sersync)

### 实时同步实践

实现web上传文件，实则是写入NFS，当NFS存储新数据时触发实时同步操作，复制到备份服务器

|角色|外网IP|内网IP|安装工具|
|---|---|---|---|
|web01|eth0:10.0.0.7|eth1:172.16.1.7|httpd、php|
|nfs-server|eth0:10.0.0.31|eth1:172.16.1.31|nfs-tuils、rsync、inotify、sersync|
|backup|eth0:10.0.0.41|eth1:172.16.1.41|rsync-server|

1. WEB上传文件至NFS

```shell
# NFS角色配置
[root@nfs ~]# yum install -y nfs-utils
[root@nfs ~]# more /etc/exports
/data/	172.16.1.0/24(rw,sync,all_squash,anonuid=666,anongid=666)
[root@nfs ~]# groupadd -g 666 www
[root@nfs ~]# useradd -u 666 -g 666 www
[root@nfs ~]# mkdir /data
[root@nfs ~]# chown -R www.www /data

# WEB角色配置
[root@web01 ~]# yum install -y httpd php nfs-utils
[root@web01 ~]# systemctl start httpd
[root@web01 ~]# firewall-cmd --add-service=http --permanent
[root@web01 ~]# firewall-cmd --reload
[root@web01 ~]# mount -t nfs 172.16.1.31:/data /var/www/html/
[root@web01 ~]# wget https://dqunying2.jb51.net/201911/yuanma/upload_jb51.rar
[root@web01 ~]# unar upload_jb51.rar
[root@web01 ~]# mv /root/upload_jb51/upload.php /var/www/html/index.php
[root@web01 ~]# groupadd -g 666 www
[root@web01 ~]# useradd -u 666 -g 666 www
[root@web01 ~]# more /etc/httpd/conf/httpd.conf | egrep -v "^$|^.*#"
...
User www
Group www
...
```

WEB端的httpd服务，其所属用户和所属组必须修改，需要与NFS的用户保持一致，否则会出现WEB端挂载NFS后，数据写入报错的情况，系统用户直接写入NFS存储没有问题，但通过网页上传文件时会报错，权限拒绝；网页默认登录密码`danbaise.com`

PHP默认限制上传文件的大小为2MB，上传文件时也可能会出现文件大小超出限制报错的情况，具体情况根据日志进行解决

2. WEB和NFS的数据都备份到BACKUP的/backup目录

```shell
# BACKUP角色配置
[root@backup ~]# yum install -y rsync
[root@backup ~]# vim /etc/rsyncd.conf
[root@backup ~]# more /etc/rsyncd.conf
uid = www
gid = www
use chroot = no
max connections = 200
timeout = 900
ignore error
fake super = yes
port = 873
read only = false
list = false
auth users = rsync_backup
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log

[backup]
        path = /backup

[data]
        path = /data

[root@backup ~]# groupadd -g 666 www
[root@backup ~]# useradd -u 666 -g 666 -M www
[root@backup ~]# more /etc/rsync.passwd
rsync_backup:Huawei@123.com
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# mkdir /data
[root@backup ~]# mkdir /backup
[root@backup ~]# chown -R www.www /backup /data
[root@backup ~]# systemctl restart rsyncd

# 任意客户端执行rsync推送数据测试服务端
[root@nfs ~]# rsync -avzP /etc/sysconfig rsync_backup@backup::backup
```

3. NFS的数据实时同步到BACKUP的/data目录

监控NFS服务器上的/data/目录，如果发生变化就触发动作，动作就是执行一次数据同步

```shell
# NFS角色配置
[root@nfs ~]# wget https://github.com/wsgzao/sersync/raw/master/sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs ~]# tar -xzf sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs ~]# mv GNU-Linux-x86/ /usr/local/sersync
[root@nfs ~]# vim /usr/local/sersync/confxml.xml
<?xml version="1.0" encoding="ISO-8859-1"?>
<head version="2.5">
    <host hostip="localhost" port="8008"></host>
    <debug start="false"/>  # 调试模式，默认不开启
    <fileSystem xfs="true"/>    # 文件系统xfs，根据系统文件系统选择是否开启
    <filter start="false">  # 不同步的过滤文件
        <exclude expression="(.*)\.svn"></exclude>
        <exclude expression="(.*)\.gz"></exclude>
        <exclude expression="^info/*"></exclude>
        <exclude expression="^static/*"></exclude>
    </filter>
    <inotify>   # 通知接口，监控目录产生的变化类型
        <delete start="true"/>  # 删除
        <createFolder start="true"/>    # 创建目录
        <createFile start="true"/>
        <closeWrite start="true"/>      # 关闭写
        <moveFrom start="true"/>
        <moveTo start="true"/>
        <attrib start="true"/>         # 属性
        <modify start="true"/>         # 修改
    </inotify>

    <sersync>   # 发生变化时触发的动作
        <localpath watch="/data">     # 被监控的本地目录
            <remote ip="172.16.1.41" name="data"/>  # 触发动作，推送到远程备份服务器的IP和模块
            <!--<remote ip="192.168.8.39" name="tongbu"/>-->
            <!--<remote ip="192.168.8.40" name="tongbu"/>-->
        </localpath>
        <rsync>     # 推送到备份服务器使用的命令
            <commonParams params="-az"/>     # 命令使用的参数
            <auth start="true" users="rsync_backup" passwordfile="/etc/rsync.passwd"/>  # 启用用户验证，并指明用于验证的账户文件
            <userDefinedPort start="false" port="874"/><!-- port=874 -->    # 是否监听端口
            <timeout start="true" time="100"/><!-- timeout=100 -->     # 超时时间
            <ssh start="false"/>    # 是否使用ssh协议，使用rsync守护进程时不使用ssh协议
        </rsync>
        <failLog path="/tmp/rsync_fail_log.sh" timeToExecute="60"/><!--default every 60mins execute once-->     # 错误日志路径，默认60分钟执行一次同步
        <crontab start="false" schedule="600"><!--600mins-->    # 定时任务设置
            <crontabfilter start="false">   # 定时任务过滤
                <exclude expression="*.php"></exclude>
                <exclude expression="info/*"></exclude>
            </crontabfilter>
        </crontab>
        <plugin start="false" name="command"/>      # 是否启用插件
    </sersync>

    <plugin name="command">     # 插件名称
        <param prefix="/bin/sh" suffix="" ignoreError="true"/>  <!--prefix /opt/tongbu/mmm.sh suffix-->
        <filter start="false">
            <include expression="(.*)\.php"/>
            <include expression="(.*)\.sh"/>
        </filter>
    </plugin>

    <plugin name="socket">  # 插件名称
        <localpath watch="/opt/tongbu">
            <deshost ip="192.168.138.20" port="8009"/>
        </localpath>
    </plugin>
    <plugin name="refreshCDN">  # 插件名称
        <localpath watch="/data0/htdocs/cms.xoyo.com/site/">    # 监控本地目录的静态文件
            <cdninfo domainname="ccms.chinacache.com" port="80" username="xxxx" passwd="xxxx"/>     # 同步推送到CDN节点
            <sendurl base="http://pic.xoyo.com/cms"/>
            <regexurl regex="false" match="cms.xoyo.com/site([/a-zA-Z0-9]*).xoyo.com/images"/>
        </localpath>
    </plugin>
</head>

[root@nfs ~]# vim /etc/rsync.passwd
Huawei@123.com
[root@nfs ~]# chmod 600 /etc/rsync.passwd
[root@nfs ~]# yum install -y inotify-tools
[root@nfs ~]# /usr/local/sersync/sersync2 -h    # 查看sersync的帮助手册
[root@nfs ~]# /usr/local/sersync/sersync2 -dro /usr/local/sersync/confxml.xml    # 按配置文件启动守护进程

# 守护进程启动后会输出具体执行的命令和参数，复制该命令手动再执行一次可以观察命令是否有问题
[root@nfs data]# cd /data && rsync -az -R --delete ./  --timeout=100 rsync_backup@172.16.1.41::data --password-file=/etc/rsync.passwd >/dev/null 2>&1    # 手动执行测试命令，观察命令是否执行成功
```

注：如果有多个目录需要监控并实时同步，*不能直接在.xml文件内增加`<localpath watch="/data">`多个目录*，**只能复制.xml配置文件，修改`<localpath watch="/data">`目录后，使用sersync2命令再启动一次新的配置文件；使用sersync2命令启动守护进程时，不要多次执行重复的命令，每执行一次sersync2守护进程，都会启动一个进程，即便命令是重复的。由于sersync没有专门的管理服务，所以只能通过类似pkill的命令停止多余的sersync2守护进程**

如果执行sersync2守护进程后，再执行测试命令发现报错，直接修改xml配置文件后，再执行一次测试命令即可，无需重复执行sersync2命令；**NFS的sersync的虚拟用户账户文件，只要写虚拟用户的密码，不要用户名，否则执行sersync同步数据到BACKUP会失败**

#### 实现数据平滑迁移

迁移NFS数据到BACKUP服务器，并将后续数据直接指向BACKUP服务器，在这个过程中还需要保证业务不中断

[![sersync业务平滑迁移](https://s1.ax1x.com/2022/12/10/zfnsOK.png)](https://imgse.com/i/zfnsOK)

1. 首先NFS的数据全部实时同步到BACKUP，实现数据的迁移，保证业务迁移时不会因为数据差异大产生较大的影响
2. BACKUP需要运行NFS上一样的业务环境，例如nfs服务
3. 在WEB上实现切换nfs服务端，卸载NFS的/data目录，挂载BACKUP服务的/data目录

```shell
# BACKUP角色配置
[root@backup ~]# groupadd -g 666 www
[root@backup ~]# useradd -u 666 -g 666 www
[root@backup ~]# yum install -y nfs-utils
[root@backup ~]# more /etc/exports
/data/  172.16.1.0/24(rw,sync,all_squash,anonuid=666,anongid=666)
[root@backup ~]# systemctl start nfs-server.service
[root@backup ~]# systemctl start rpcbind
[root@backup ~]# firewall-cmd --add-service=nfs --permanent
[root@backup ~]# firewall-cmd --add-service=mountd --permanent
[root@backup ~]# firewall-cmd --add-service=rpc-bind --permanent
[root@backup ~]# firewall-cmd reload

# WEB角色配置
[root@web01 ~]# umount /var/www/html && mount -t nfs 172.16.1.41:/data /var/www/html
```

# SSH

ssh进行数据传输前会先对数据包进行加密，确保数据传输安全，除了ssh协议能够提供远程连接服务，telnet同样能够实现远程连接功能，ssh默认监听在本地的`22/tcp`端口、支持root用户登录，telnet不对传输数据加密，监听在本地`23/tcp`端口、默认不支持root用户登录

[SSH协议握手核心过程](https://www.bilibili.com/video/BV13P4y1o76u/?spm_id_from=333.788&vd_source=a4d23142d8450189ec49b539782f3b74)

## ssh与telnet对比

1. 安装telnet服务端

```shell
[root@backup ~]# yum install -y telnet-server
[root@backup ~]# systemctl start telnet.socket
[root@backup ~]# ss -tunpl | column -t
tcp    LISTEN  0       128     [::]:23        [::]:*        users:(("systemd",pid=1,fd=28))
[root@backup ~]# useradd hebor      # 创建telnet测试用户
[root@backup ~]# echo "redhat" | passwd --stdin hebor
```

通过`ss`命令看到telnet服务是由systemd启动的，不是由telnet本身启动

2. 通过wireshark查看telnet流量

[![捕获telnet包-1](https://s1.ax1x.com/2022/12/12/z4NDCq.png)](https://imgse.com/i/z4NDCq)

[![捕获telnet包-2](https://s1.ax1x.com/2022/12/12/z4N05n.png)](https://imgse.com/i/z4N05n)

3. 通过wireshark查看ssh流量

[![捕获ssh包-1](https://s1.ax1x.com/2022/12/12/z4NOVH.png)](https://imgse.com/i/z4NOVH)

[![捕获ssh包-2](https://s1.ax1x.com/2022/12/12/z4Nqqe.png)](https://imgse.com/i/z4Nqqe)

## SSH相关命令

SSH属于C/S架构，其包含客户端和服务端，在客户端中包含ssh远程登录、scp远程拷贝、sftp文件传输、ssh-copy-id密钥分发等应用程序

1. ssh远程登录

```shell
ssh -p 22 root@172.16.1.41

# -p 指定连接端口，默认22端口可省略
# “@”前面为用户名，默认以你系统当前登录的用户名登录
```

2. scp远程拷贝（全量拷贝）

```shell
[root@backup ~]# scp root@172.16.1.31:/etc/exports /opt/    # 拉取
[root@backup ~]# scp /etc/hosts root@172.16.1.31:/opt/      # 推送

# -l 限制传输使用带宽（默认Kb）
# -p 拷贝文件前后保持文件或目录属性不变

[root@nfs ~]# time scp /etc/ root@172.16.1.41:/opt/
real    0m3.669s    # 实际时间，命令从开始执行到运行终止的时间
user    0m0.002s    # 用户CPU时间，命令在用户态中执行的时间总和
sys     0m0.003s    # 系统CPU时间，命令在核心态中执行的时间总和
```

scp每拷贝一个文件都会建立一个TCP连接，所以在拷贝大量小文件的情况下，scp拷贝速度会非常慢；time命令用于计算命令执行的时间，计算直接拷贝`/etc/`目录的时间，与将`/etc/`目录打包后的拷贝时间做对比，将`/etc/`目录打包后传输，只会建立一次TCP连接，传输速度更快

关于`-l`选项，默认传输单位使用Kb，此处的`b`表示`bit`，所以将其转换为KB再转换成MB需要`Kb / 8 / 1024`

3. sftp远程数据传输

```shell
[root@nfs ~]# sftp root@172.16.1.41
[root@nfs ~]# sftp -o Port=52113 root@172.16.1.41
sftp> get anaconda-ks.cfg   # 拉取文件
```

### ssh验证方式

1. 基于账户密码远程登录

2. 基于密钥远程登录

默认情况下，通过ssh客户端命令登录远程服务器，需要提供远程服务器上的账号密码，为了降低密码泄露率，建议使用密钥验证方式

#### ssh密钥认证过程

在聊ssh密钥认证过程前，需要先了解一下两种加密方式：*对称加密*和*非对称加密*，对称加密指的是，C（Client）端和S（Server）端使用相同的*一个密钥进行加密或解密*；非对称加密指的是*加密和解密所使用的密钥不是同一个*，ssh是建立在应用层基础上的安全协议，使用非对称加密

ssh会产生一对密钥，分别是公钥和私钥，C端将公钥发送给S端，S端可以通过C端的公钥加密数据后再发送给C端，只有C端的私钥才能够对该公钥进行解密

ssh密钥认证过程：

1. 使用ssh命令连接远程主机，执行TCP三次握手
2. ssh握手，双方互相发送ssh协议版本；有SSH1和SSH2两个版本，两个版本不兼容，加密方式也不相同，通过wireshark抓包可以看到双方互相发送协议版本，另外SSH-1.99字样表示发送方支持SSH的两种版本
3. 密钥交换初始化（Key Exchange Init），双方协商使用哪种算法来拿到密钥
4. ECDH密钥交换初始化，抓包（Elliptic Curve Diffie-Hellman Key Exchange Init）还能够看到C端已经给S端发送了个公钥；此时C端发送的密钥只是**客户端临时密钥**，这对临时密钥是用来后续生成**共享安全密钥**的，一般临时密钥也会进行销毁，进一步提升加密的安全性，S端也会生成自己的临时密钥对，也是用来生成共享密钥对的
5. S端使用自己的临时密钥对（临时公钥、临时私钥）和C端发送过来的临时公钥，三者组合生成共享安全密钥
6. S端再生成一对host密钥对，host密钥对是用于验证S端身份的
7. S端使用共享安全密钥和其他信息来生成一串**交换哈希值**；为了确保管理员与服务器之间的ssh认证是正常的，需要通过哈希值对共享安全密钥进行校验，确保认证信息没有被第三方篡改，最简单的方式就是C端与S端生成同样的哈希值就能够证明密钥信息是否被篡改了
8. S端将生成的交换哈希值**使用host私钥进行加密（签名）**
9. EDCH密钥交换回复，抓包（Elliptic Curve Diffie-Hellman Key Exchange Reply）可以看到S端临时公钥、S端host公钥、交换哈希值
10. C端收到S端的临时公钥后也能够生成同样的共享安全密钥，因此，C端会和S端使用同样的方式生成一串C端的交换哈希值
11. C端使用S端的host公钥对S端的交换哈希值进行解密
12. 对比S端的交换哈希值和C端的交换哈希值，如果值一样证明SSH认证信息没有经过第三方篡改
13. 新密钥（New Kyes），双方都有这个操作，双方都需要生成6个密钥，其中 2个数据加密密钥、2个数据完整性密钥、2个加强对称密钥；每2个密钥实现一组功能，能够使第三方更难解密出原始数据，同时，对同样的信息加密两次，更不容易出现相同的密文
14. 有了6个密钥后，后面传输的信息都是加密数据了

生成交换哈希值所需要用到的信息：C端与S端的版本号信息（SSH握手信息）、密钥交换初始化阶段双方发送的算法名称的字符串、服务端host公钥、客户端临时公钥、服务端临时公钥、双方的共享安全密钥

虽然使用哈希值验证的方式能够加强SSH认证的安全性，但是第三方也不是毫无办法，第三方完全有可能从一开始就截获了用户与服务器之间的对话，有可能用户一直都是在与第三方生成共享安全密钥，所以第一次或者之前连接过服务器，一般会有提示是否可相信对端发送过来的host公钥，因为会和本地主机数据做对比，没出现过或者有更改就产生了提示，如果输入yes，下一步就是输入密码，如果此时对话被监听，密码就直接会被窃取；由于不知道用户什么时候会连接服务器，第三方一般也不会一直等待监听，因此第一次用密码登录相对是比较安全的，但是如果频繁这样操作，安全问题就会被放大了

### SSH安全设置

1. 修改远程连接端口
2. 禁止Root用户登录
3. 使用密钥认证方式
4. 重要服务不使用公网IP
5. 防火墙限制来源IP

```shelll
[root@m01 ~]# vi /etc/ssh/sshd_config
Port 220                    # 变更SSH服务远程连接端口
PermitRootLogin no          # 禁止Root用户远程登录
PasswordAuthentication no   # 禁止使用密码远程登录
UseDNS no                   # 禁止SSH进行DNS反向解析，影响SSH连接效率参数
GSSAPIAuthentication no     # 禁止GSS认证，减少连接时产生的延迟
RSAAuthentication yes       # 启用公钥认证，此选项默认不用修改
PubkeyAuthentication yes    # 启用公钥认证
```

1. 管理员使用任意系统（Windows、Linux、Mac）和工具（Git、xshell）在客户端主机生成密钥对
2. 从客户端复制公钥内容到jumpserver的`~/.ssh/authorized_keys`文件中
3. jumpserver生成密钥对，推送公钥到内网主机

```shell
# 1. 生成密钥对
[root@m01 ~]# ssh-keygen -C hebor@t450s.com
    # -t 指定加密类型
    # -C 指定用户邮箱

# 2. 推送公钥到目标主机
[root@m01 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.16.1.31

# 3. 测试连接目标主机是否需要密码
```

#### 禁止特定条件登录

针对不想禁止所有用户使用密码登录的情况，可以通过修改ssh配置文件的Match模块进行限制，如果Match所在行的条件匹配成功，则Match区块下所有的关键字将被逐个加载，直到遇见另一个Match关键字或者文件结尾。所以一般Match区块添加在sshd_config文件末尾

示例：禁止用户foo、用户组bar使用密码登录

```shell
Match User foo, Group bar       # Match条件
    PasswordAuthentication no   # 关键字
```

示例：禁止除用户foo以外的其他用户使用密码

```shell
Match User *, !foo
    PasswordAuthentication no
```

Match支持的关键字与SSH配置文件的许多选项一致

#### fail2ban工具

fail2ban可以监控系统日志，并根据一定规则匹配异常IP后使用Firewalld将其屏蔽，尤其针对一些爆破/扫描等手段非常有效

```shell
# 1. 开启Firewalld防火墙
systemctl start firewalld.service
systemctl enable firewalld

# 2. 修改Firewalld放行规则，Firewalld默认仅放行ssh服务的22号端口
firewall-cmd --permanent --add-service=ssh --add-service=http
firewall-cmd --reload

# 3. 安装fail2ban，需要epel源
yum install -y fail2ban fail2ban-firewalld mailx

# 4. 配置fail2ban规则，.local会覆盖.conf文件
vim /etc/fail2ban/jail.local
[DEFAULT]
ignoreip = 127.0.0.1/8
bantime = 86400
findtime = 600
maxretry = 5
banaction = firewallcmd-ipset
action = %(action_mwl)s

[sshd]
enabled = true
filter  = sshd
port    = 2022
action  = %(action_mwl)s
logpath = /var/log/secure

# 5. 启动fail2ban服务，检查ssh状态
systemctl start fail2ban.service
fail2ban-client status sshd

# 6. 移除被封禁的IP，使该IP又重新可以登录
fail2ban-client set sshd unbanip 10.0.0.1
```

# JumpServer

[![jumpserver](https://s1.ax1x.com/2022/12/16/z78uHe.png)](https://imgse.com/i/z78uHe)

跳板机与堡垒机的区别：跳板机实现了对后端服务器的统一管理和登录安全，但是没有实现对管理员的行为监控和审计，在使用跳板机的过程中仍有可能在服务器上进行错误操作；堡垒机等同于跳板机的升级版，它对管理员的操作进行监控和审计，其运维思想是从源头上降低服务器误操作的事故

几乎所有的企业都需要堡垒机，堡垒机是企业进行*资产管理、运维安全审计*的重要组件，只不过由于企业的资金问题对堡垒机的投入也不一样

堡垒机的4A能力

1. Authentication（身份鉴别）：防止身份冒用和复用
2. Authorization（授权控制）：防止内部误操作和权限滥用
3. Accounting（账号管理）：人员和资产的管理
4. Auditing（安全审计）：追溯的保障和事故分析的依据

## JumpServer的构成

- LINA：新的前端模块，负责页面的展现
- Luna：现在是Web Terminal前端，计划前端页面都由该项目提供，Jumpserver只提供API，不在负责后台渲染html等
- CORE：现指JumpServer管理后台，是核心组件，使用Django Class Based View峰哥开发，支持Restful API
- CoCo/KoKo：实现了SSH Server和Web Terminal Server的组件，提供SSH和WebSocket接口，使用Paramiko和Flask开发
- Guacamole：Apache跳板机项目，Jumpserver使用其组件实现RDP功能，Jumpserver并没有修改其代码而是添加了额外的插件，支持Jumpserver调用

### JumpServer安装

#### 一、 安装环境评估

1. 主机容量评估：根据预期的资产规模和同时并发用户数量，评估安装Jumpserver的虚拟机的CPU、内存
2. 存储空间评估：Jumpserver会存储大量审计录像比较占用存储空间，如果不能使用外部的分布式存储、对象存储等扩展性比较好的存储，那么在安装之前就需要对存储空间做好规划
3. 复杂性评估：是否需要高可用？是否要独立部署某个组件

[JumpServer安装文档](https://docs.jumpserver.org/zh/master/install/setup_by_fast/)

[JumpServer使用教程](https://www.bilibili.com/video/BV19D4y1S7s4?p=1)